# Lexer: Handcodierte Implementierung

> [!IMPORTANT]
>
> <details open>
>
> <summary><strong>üéØ TL;DR</strong></summary>
>
> <picture><source media="(prefers-color-scheme: light)" srcset="images/architektur_cb_lexer_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/architektur_cb_lexer_dark.png"><img src="images/architektur_cb_lexer.png"></picture>
>
> Der Lexer (auch ‚ÄúScanner‚Äù) soll den Zeichenstrom in eine Folge von
> Token zerlegen. Zur Spezifikation der Token werden regul√§re Ausdr√ºcke
> verwendet.
>
> Von Hand implementierte Lexer arbeiten √ºblicherweise rekursiv und
> verarbeiten immer das n√§chste Zeichen im Eingabestrom. Die
> Arbeitsweise erinnert an LL-Parser (vgl.
> [LL-Parser](../02-parsing/ll-parser-impl.md)).
>
> Lexer m√ºssen sehr effizient sein, da sie noch direkt auf der
> niedrigsten Abstraktionsstufe arbeiten und u.U. oft durchlaufen
> werden. Deshalb setzt man hier gern spezielle Techniken wie Puffern
> von Zeichen √ºber einen Doppel-Puffer ein.
>
> Die Palette an Fehlerbehandlungsstrategien im Lexer reichen von
> ‚Äúaufgeben‚Äù √ºber den ‚ÄúPanic Mode‚Äù (‚Äúgobbeln‚Äù von Zeichen, bis wieder
> eines passt) und Ein-Schritt-Transformationen bis hin zu speziellen
> Lexer-Regeln, die beispielsweise besonders h√§ufige Typos abfangen.
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>üé¶ Videos</strong></summary>
>
> - [VL Handcodierte Lexer](https://youtu.be/N0WJQ4UkXkM)
>
> </details>

## Lexer: Erzeugen eines Token-Stroms aus einem Zeichenstrom

Aus dem Eingabe(-quell-)text

``` c
/* demo */
a= [5  , 6]     ;
```

erstellt der Lexer (oder auch Scanner genannt) eine Sequenz von Token:

    <ID, "a"> <ASSIGN> <LBRACK> <NUM, 5> <COMMA> <NUM, 6> <RBRACK> <SEMICOL>

## Manuelle Implementierung: Rekursiver Abstieg

``` python
def nextToken():
    while (peek != EOF):  # globale Variable, √ºber consume()
        switch (peek):
            case ' ': case '\t': case '\n': WS(); continue
            case '[': consume(); return Token(LBRACK, '[')
            ...
            default:
                if isLetter(peek): return NAME()
                raise Error("invalid character: "+peek)
    return Token(EOF_Type, "<EOF>")

def WS():
    while (peek == ' ' || peek == '\t' || ...): consume()

def NAME():
    buf = StringBuilder()
    do { buf.append(peek); consume(); } while (isLetter(peek))
    return Token(NAME, buf.toString())
```

Die manuelle Implementierung ‚Äúdenkt‚Äù nicht in den Zust√§nden des DFA,
sondern orientiert sich immer am aktuellen Zeichen ‚Äú`peek`‚Äù. Abh√§ngig
von dessen Auspr√§gung wird entweder direkt ein Token erzeugt und das
Zeichen aus dem Eingabestrom entfernt sowie das n√§chste Zeichen
eingelesen (mittels der Funktion `consume()`, nicht dargestellt im
Beispiel), oder man ruft weitere Funktionen auf, die das Gew√ºnschte
erledigen, beispielsweise um White-Spaces zu entfernen oder um einen
Namen einzulesen: Nach einem Buchstaben werden alle folgenden Buchstaben
dem Namen (Bezeichner) hinzugef√ºgt. Sobald ein anderes Zeichen im
Eingabestrom erscheint, wird das Namen-Token erzeugt.

Die Funktion `consume()` ‚Äúverbraucht‚Äù das aktuelle Zeichen ‚Äú`peek`‚Äù und
holt das n√§chste Zeichen aus dem Eingabestrom.

*Anmerkung*: H√§ufig findet man im Lexer keinen ‚Äúsch√∂nen‚Äù
objektorientierten Ansatz. Dies ist i.d.R. Geschwindigkeitsgr√ºnden
geschuldet ‚Ä¶

## Read-Ahead: Unterscheiden von ‚Äú*\<*‚Äù und ‚Äú*\<=*‚Äù

``` python
def nextToken():
    while (peek != EOF):  # globale Variable
        switch (peek):
            case '<':
                if match('='): consume(); return Token(LE, "<=")
                else: consume(); return Token(LESS, '<')
            ...
    return Token(EOF_Type, "<EOF>")

def match(c):   # Lookahead: Ein Zeichen
    consume()
    if (peek == c): return True
    else: rollBack(); return False
```

Um die Token ‚Äú`<`‚Äù und ‚Äú`<=`‚Äù unterscheiden zu k√∂nnen, m√ºssen wir ein
Zeichen vorausschauen: Wenn nach dem ‚Äú`<`‚Äù noch ein ‚Äú`=`‚Äù kommt, ist es
‚Äú`<=`‚Äù, sonst ‚Äú`<`‚Äù.

Erinnerung: Die Funktion `consume()` liest das n√§chste Zeichen aus dem
Eingabestrom und speichert den Wert in der globalen Variable `peek`.

F√ºr das Read-Ahead wird die Funktion `match()` definiert, die zun√§chst
das bereits bekannte Zeichen, in diesem Fall das ‚Äú`<`‚Äù durch das n√§chste
Zeichen im Eingabestrom ersetzt (Aufruf von `consume()`). Falls der
Vergleich des Lookahead-Zeichens mit dem gesuchten Zeichen erfolgreich
ist, liegt das ‚Äúgr√∂√üere‚Äù Token vor, also ‚Äú`<=`‚Äù. Dann wird noch das
‚Äú`=`‚Äù durch das n√§chste Zeichen ersetzt und das Token `LE` gebildet.
Anderenfalls muss das zuviel gelesene Zeichen wieder in den Eingabestrom
zur√ºckgelegt werden (`rollBack()`).

## Puffern des Input-Stroms: Double Buffering

Das Einlesen einzelner Zeichen f√ºhrt zwar zu eleganten algorithmischen
L√∂sungen, ist aber zur Laufzeit deutlich ‚Äúteurer‚Äù als das Einlesen mit
gepufferten I/O-Operationen, die eine ganze Folge von Zeichen einlesen
(typischerweise einen ganzen Disk-Block, beispielsweise 4096 Zeichen).

Dazu kann man einen Ringpuffer nutzen, den man mit Hilfe von zwei gleich
gro√üen `char`-Puffern mit jeweils der L√§nge $`N`$ simulieren kann.
($`N`$ sollte dann der L√§nge eines Disk-Blocks entsprechen.)

Vergleiche auch [Wikipedia: ‚ÄúCircular
Buffer‚Äù](https://en.wikipedia.org/wiki/Circular_buffer).

<picture><source media="(prefers-color-scheme: light)" srcset="images/doublebuffer_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/doublebuffer_dark.png"><img src="images/doublebuffer.png" width="65%"></picture>

``` python
start = 0; end = 0; fill(buffer[0:n])

def consume():
    peek = buffer[start]
    start = (start+1) mod 2n
    if (start mod n == 0):
        fill(buffer[start:start+n-1])
        end = (start+n) mod 2n

def rollBack():
    if (start == end): raise Error("roll back error")
    start = (start-1) mod 2n
```

Zun√§chst wird nur der vordere Pufferteil durch einen passenden
Systemaufruf gef√ºllt.

Beim Weiterschalten im simulierten DFA oder im manuell kodierten Lexer
(Funktionsaufruf von `consume()`) wird das n√§chste Zeichen aus dem
vorderen Pufferteil zur√ºckgeliefert. √úber die Modulo-Operation bleibt
der Pointer `start` immer im Speicherbereich der beiden Puffer.

Wenn man das Ende des vorderen Puffers erreicht, wird der hintere Puffer
mit einem Systemaufruf gef√ºllt. Gleichzeitig wird ein Hilfspointer `end`
auf den Anfang des vorderen Puffers gesetzt, um Fehler beim Roll-Back zu
erkennen.

Wenn man das Ende des hinteren Puffers erreicht, wird der vordere Puffer
nachgeladen und der Hilfspointer auf den Anfang des hinteren Puffers
gesetzt.

Im Grunde ist also immer ein Puffer der ‚ÄúArbeitspuffer‚Äù und der andere
enth√§lt die bereits gelesene (verarbeitete) Zeichenkette. Wenn beim
Nachladen weniger als $`N`$ Zeichen gelesen werden, liefert der
Systemaufruf als letztes ‚ÄúZeichen‚Äù ein `EOF`. Beim Verarbeiten wird
`peek` entsprechend diesen Wert bekommen und der Lexer muss diesen Wert
abfragen und ber√ºcksichtigen.

F√ºr das Roll-Back wird der `start`-Pointer einfach dekrementiert (und
mit einer Modulo-Operation auf den Speicherbereich der beiden Puffer
begrenzt). Falls dabei der `end`-Pointer ‚Äúeingeholt‚Äù wird, ist der
`start`-Pointer durch beide Puffer zur√ºckgelaufen und es gibt keinen
fr√ºheren Input mehr. In diesem Fall wird entsprechend ein Fehler
gemeldet.

*Anmerkung*: In der Regel sind die Lexeme kurz und man muss man nur ein
bis zwei Zeichen im Voraus lesen. Dann ist eine Puffergr√∂√üe von 4096
Zeichen mehr als ausreichend gro√ü und man sollte nicht in Probleme
laufen. Wenn der n√∂tige Look-Ahead aber beliebig gro√ü werden kann, etwa
bei Sprachen ohne reservierte Schl√ºsselw√∂rtern oder bei
Kontext-sensitiven Lexer-Grammatiken (denken Sie etwa an die
Einr√ºcktiefe bei Python), muss man andere Strategien verwenden. ANTLR
beispielsweise vergr√∂√üert in diesem Fall den Puffer dynamisch,
alternativ k√∂nnte man die Aufl√∂sung zwischen Schl√ºsselw√∂rtern und
Bezeichnern dem Parser √ºberlassen.

## Typische Muster f√ºr Erstellung von Token

1.  Schl√ºsselw√∂rter

    - Ein eigenes Token (RE/DFA) f√ºr jedes Schl√ºsselwort, oder
    - Erkennung als Name und Vergleich mit W√∂rterbuch und nachtr√§gliche
      Korrektur des Tokentyps

    Wenn Schl√ºsselw√∂rter √ºber je ein eigenes Token abgebildet werden,
    ben√∂tigt man f√ºr jedes Schl√ºsselwort einen eigenen RE bzw. DFA. Die
    Erkennung als Bezeichner und das Nachschlagen in einem W√∂rterbuch
    (geeignete Hashtabelle) sowie die entsprechende nachtr√§gliche
    Korrektur des Tokentyps kann die Anzahl der Zust√§nde im Lexer
    signifikant reduzieren!

2.  Operatoren

    - Ein eigenes Token f√ºr jeden Operator, oder
    - Gemeinsames Token f√ºr jede Operatoren-Klasse

3.  Bezeichner: Ein gemeinsames Token f√ºr alle Namen

4.  Zahlen: Ein gemeinsames Token f√ºr alle numerischen Konstante (ggf.
    Integer und Float unterscheiden)

    F√ºr Zahlen f√ºhrt man oft ein Token ‚Äú`NUM`‚Äù ein. Als Attribut
    speichert man das Lexem i.d.R. als String. Alternativ kann man
    (zus√§tzlich) das Lexem in eine Zahl konvertieren und als
    (zus√§tzliches) Attribut speichern. Dies kann in sp√§teren Stufen viel
    Arbeit sparen.

5.  String-Literale: Ein gemeinsames Token

6.  Komma, Semikolon, Klammern, ‚Ä¶: Je ein eigenes Token

<!-- -->

1.  Regeln f√ºr White-Space und Kommentare etc. ‚Ä¶

    Normalerweise ben√∂tigt man Kommentare und White-Spaces in den
    folgenden Stufen nicht und entfernt diese deshalb aus dem
    Eingabestrom. Dabei k√∂nnte man etwa White-Spaces in den Pattern der
    restlichen Token ber√ºcksichtigen, was die Pattern aber sehr komplex
    macht. Die Alternative sind zus√§tzliche Pattern, die auf die
    White-Space und anderen nicht ben√∂tigten Inhalt matchen und diesen
    ‚Äúger√§uschlos‚Äù entfernen. Mit diesen Pattern werden keine Token
    erzeugt, d.h. der Parser und die anderen Stufen bemerken nichts von
    diesem Inhalt.

    Gelegentlich ben√∂tigt man aber auch Informationen √ºber White-Spaces,
    beispielsweise in Python. Dann m√ºssen diese Token wie normale Token
    an den Parser weitergereicht werden.

Jedes Token hat i.d.R. ein Attribut, in dem das Lexem gespeichert wird.
Bei eindeutigen Token (etwa bei eigenen Token je Schl√ºsselwort oder bei
den Interpunktions-Token) kann man sich das Attribut auch sparen, da das
Lexem durch den Tokennamen eindeutig rekonstruierbar ist.

| Token | Beschreibung | Beispiel-Lexeme |
|:---|:---|:---|
| `if` | Zeichen `i` und `f` | `if` |
| `relop` | `<` oder `>` oder `<=` oder `>=` oder `==` oder `!=` | `<`, `<=` |
| `id` | Buchstabe, gefolgt von Buchstaben oder Ziffern | `pi`, `count`, `x3` |
| `num` | Numerische Konstante | `42`, `3.14159`, `0` |
| `literal` | Alle Zeichen au√üer `"`, in `"` eingeschlossen | `"core dumped"` |

*Anmerkung*: Wenn es mehrere matchende REs gibt, wird in der Regel das
l√§ngste Lexem bevorzugt. Wenn es mehrere gleich lange Alternativen gibt,
muss man mit Vorrangregeln bzgl. der Token arbeiten.

## Fehler bei der Lexikalischen Analyse

Problem: Eingabestrom sieht so aus: `fi (a==42) { ... }`

Der Lexer kann nicht erkennen, ob es sich bei `fi` um ein vertipptes
Schl√ºsselwort handelt oder um einen Bezeichner: Es k√∂nnte sich um einen
Funktionsaufruf der Funktion `fi()` handeln ‚Ä¶ Dieses Problem kann erst
in der n√§chsten Stufe sinnvoll erkannt und behoben werden.

=\> Was tun, wenn keines der Pattern auf den Anfang des Eingabestroms
passt?

Optionen:

- Aufgeben ‚Ä¶

  Eventuell vielleicht sogar die beste und einfachste Variante :-)

<!-- -->

- ‚ÄúPanic Mode‚Äù: Entferne so lange Zeichen, bis ein Pattern passt.

  Das verwirrt u.U. den Parser, kann aber insbesondere in interaktiven
  Umgebungen hilfreich sein. Ggf. kann man dem Parser auch
  signalisieren, dass hier ein Problem vorlag.

<!-- -->

- Ein-Schritt-Transformationen:
  - F√ºge fehlendes Zeichen in Eingabestrom ein.
  - Entferne ein Zeichen aus Eingabestrom.
  - Vertausche ein Zeichen:
    - Ersetze ein Zeichen durch ein anderes.
    - Vertausche zwei benachbarte Zeichen.

  Diese Transformationen versuchen, den Input in einem Schritt zu
  reparieren. Das ist durchaus sinnvoll, da in der Praxis die meisten
  Fehler in dieser Stufe durch ein einzelnes Zeichen hervorgerufen
  werden: Es fehlt ein Zeichen oder es ist eines zuviel im Input. Es
  liegt ein falsches Zeichen vor (Tippfehler) oder zwei benachbarte
  Zeichen wurden verdreht ‚Ä¶

  Im Prinzip k√∂nnte man auch eine allgemeinere Strategie versuchen,
  indem man diejenige Transformation mit der *kleinsten Anzahl von
  Schritten* zur Fehlerbehebung bestimmt. Beispiele daf√ºr finden sich im
  Bereich Natural Language Processing (*NLP*), etwa die
  Levenshtein-Distanz oder der SoundEx-Algorithmus oder sogar
  Hidden-Markov-Modelle. Allerdings muss man sich in Erinnerung rufen,
  dass gerade in dieser ersten Phase eines Compilers die Geschwindigkeit
  stark im Fokus steht und eine ausgefeilte Fehlerkorrekturstrategie die
  vielen kleinen Optimierungen schnell wieder zunichte machen kann.

<!-- -->

- Fehler-Regeln: Matche typische Typos

  Gelegentlich findet man in den Grammatiken f√ºr den Lexer extra Regeln,
  die h√§ufige bzw. typische Typos matchen und dann passend darauf
  reagieren.

## Wrap-Up

- Zusammenhang DFA, RE und Lexer

<!-- -->

- Implementierungsansatz: Manuell codiert (rekursiver Abstieg)

<!-- -->

- Read-Ahead
- Puffern mit Doppel-Puffer-Strategie

<!-- -->

- Typische Fehler beim Scannen

## üìñ Zum Nachlesen

- Aho u.¬†a. ([2023](#ref-Aho2023)): Abschnitt 2.6 und Kapitel 3
- Torczon und Cooper ([2012](#ref-Torczon2012)): Kapitel 2
- Mogensen ([2017](#ref-Mogensen2017)): Kapitel 1 (insbesondere
  Abschnitt 1.8)

> [!NOTE]
>
> <details>
>
> <summary><strong>‚úÖ Lernziele</strong></summary>
>
> - k1: Ich kenne die Aufgaben eines Lexers
> - k2: Ich kann die manuelle Implementierung eines Lexers mit Hilfe von
>   rekursivem Abstieg erkl√§ren
> - k2: Ich kann den Umgang mit dem Doppel-Puffer erkl√§ren
> - k2: Ich kann die Varianten bei der Erkennung von Schl√ºsselw√∂rtern an
>   einem Beispiel verdeutlichen
> - k2: Ich kann typische Fehler und L√∂sungsans√§tze in der lexikalischen
>   Analyse erkl√§ren
> - k3: Ich kann f√ºr ein Problem eine typische Einteilung von Token
>   vornehmen
> - k3: Ich kann einen Top-Down-Lexer mit Read-Ahead und intelligenter
>   Pufferung implementieren
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>üëÄ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent"
> entry-spacing="0">
>
> <div id="ref-Aho2023" class="csl-entry">
>
> Aho, A. V., M. S. Lam, R. Sethi, J. D. Ullman, und S. Bansal. 2023.
> *Compilers: Principles, Techniques, and Tools, Updated 2nd Edition by
> Pearson*. Pearson India.
> <https://learning.oreilly.com/library/view/compilers-principles-techniques/9789357054881/>.
>
> </div>
>
> <div id="ref-Mogensen2017" class="csl-entry">
>
> Mogensen, T. 2017. *Introduction to Compiler Design*. Springer.
> <https://doi.org/10.1007/978-3-319-66966-3>.
>
> </div>
>
> <div id="ref-Torczon2012" class="csl-entry">
>
> Torczon, L., und K. Cooper. 2012. *Engineering a Compiler*. Morgan
> Kaufmann.
> <https://learning.oreilly.com/library/view/engineering-a-compiler/9780080916613/>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> dc5f900 (lecture: use local files for attachments (Recursive), 2025-10-15)<br></sub></sup></p></blockquote>
